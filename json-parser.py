import json

# Sample JSON data (replace with actual JSON)
data = {
"metrics": {
        "model=detectron2_fasterrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 272.2146,
        "model=detectron2_fasterrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.0495488,
        "model=detectron2_fasterrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6.217129,
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.66393,
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 121.608503,
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.741941,
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 217.595466,
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 78.404367,
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 86.942107,
        "model=detectron2_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=detectron2_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 22.3719,
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.64432,
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1463.578985,
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 103.31959,
        "model=detectron2_fasterrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=detectron2_fasterrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 254.96926,
        "model=detectron2_fasterrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=detectron2_fasterrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 18.695763,
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 11.866698,
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6060.305282,
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.217894400000006,
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "(\"<class 'torch._dynamo.exc.BackendCompilerFailed'>\", '(\\'backend=\\\\\\'inductor\\\\\\' raised:\\\\nCalledProcessError: Command \\\\\\'[\\\\\\'/usr/bin/g++\\\\\\', \\\\\\'--std=c++17\\\\\\', \\\\\\'/tmp/tmp1h0qxsy3/main.cpp\\\\\\', \\\\\\'-O3\\\\\\', \\\\\\'-shared\\\\\\', \\\\\\'-fPIC\\\\\\', \\\\\\'-Wno-psabi\\\\\\', \\\\\\'-o\\\\\\', \\\\\\'/tmp/tmp1h0qxsy3/spirv_utils.cpython-312-x86_64-linux-gnu.so\\\\\\', \\\\\\'-lze_loader\\\\\\', \\\\\\'-lsycl\\\\\\', \\\\\\'-L/home/edward/miniconda3/envs/pytorch/lib\\\\\\', \\\\\\'-L/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/triton/backends/intel/lib\\\\\\', \\\\\\'-I/usr/local/include\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include/sycl\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/triton/backends/intel/include\\\\\\', \\\\\\'-I/tmp/tmp1h0qxsy3\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include/python3.12\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/numpy/_core/include\\\\\\', \\\\\\'-Wl,-rpath,/home/edward/miniconda3/envs/pytorch/lib\\\\\\']\\\\\\' returned non-zero exit status 1.\\\\n\\\\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\\\\n\\\\n\\\\nYou can suppress this exception and fall back to eager by setting:\\\\n    import torch._dynamo\\\\n    torch._dynamo.config.suppress_errors = True\\\\n\\',)')",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "(\"<class 'torch._dynamo.exc.BackendCompilerFailed'>\", '(\\'backend=\\\\\\'inductor\\\\\\' raised:\\\\nCalledProcessError: Command \\\\\\'[\\\\\\'/usr/bin/g++\\\\\\', \\\\\\'--std=c++17\\\\\\', \\\\\\'/tmp/tmp1h0qxsy3/main.cpp\\\\\\', \\\\\\'-O3\\\\\\', \\\\\\'-shared\\\\\\', \\\\\\'-fPIC\\\\\\', \\\\\\'-Wno-psabi\\\\\\', \\\\\\'-o\\\\\\', \\\\\\'/tmp/tmp1h0qxsy3/spirv_utils.cpython-312-x86_64-linux-gnu.so\\\\\\', \\\\\\'-lze_loader\\\\\\', \\\\\\'-lsycl\\\\\\', \\\\\\'-L/home/edward/miniconda3/envs/pytorch/lib\\\\\\', \\\\\\'-L/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/triton/backends/intel/lib\\\\\\', \\\\\\'-I/usr/local/include\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include/sycl\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/triton/backends/intel/include\\\\\\', \\\\\\'-I/tmp/tmp1h0qxsy3\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include/python3.12\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/numpy/_core/include\\\\\\', \\\\\\'-Wl,-rpath,/home/edward/miniconda3/envs/pytorch/lib\\\\\\']\\\\\\' returned non-zero exit status 1.\\\\n\\\\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\\\\n\\\\n\\\\nYou can suppress this exception and fall back to eager by setting:\\\\n    import torch._dynamo\\\\n    torch._dynamo.config.suppress_errors = True\\\\n\\',)')",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "(\"<class 'torch._dynamo.exc.BackendCompilerFailed'>\", '(\\'backend=\\\\\\'inductor\\\\\\' raised:\\\\nCalledProcessError: Command \\\\\\'[\\\\\\'/usr/bin/g++\\\\\\', \\\\\\'--std=c++17\\\\\\', \\\\\\'/tmp/tmp1h0qxsy3/main.cpp\\\\\\', \\\\\\'-O3\\\\\\', \\\\\\'-shared\\\\\\', \\\\\\'-fPIC\\\\\\', \\\\\\'-Wno-psabi\\\\\\', \\\\\\'-o\\\\\\', \\\\\\'/tmp/tmp1h0qxsy3/spirv_utils.cpython-312-x86_64-linux-gnu.so\\\\\\', \\\\\\'-lze_loader\\\\\\', \\\\\\'-lsycl\\\\\\', \\\\\\'-L/home/edward/miniconda3/envs/pytorch/lib\\\\\\', \\\\\\'-L/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/triton/backends/intel/lib\\\\\\', \\\\\\'-I/usr/local/include\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include/sycl\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/triton/backends/intel/include\\\\\\', \\\\\\'-I/tmp/tmp1h0qxsy3\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/include/python3.12\\\\\\', \\\\\\'-I/home/edward/miniconda3/envs/pytorch/lib/python3.12/site-packages/numpy/_core/include\\\\\\', \\\\\\'-Wl,-rpath,/home/edward/miniconda3/envs/pytorch/lib\\\\\\']\\\\\\' returned non-zero exit status 1.\\\\n\\\\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\\\\n\\\\n\\\\nYou can suppress this exception and fall back to eager by setting:\\\\n    import torch._dynamo\\\\n    torch._dynamo.config.suppress_errors = True\\\\n\\',)')",
        "model=detectron2_fcos_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 78.169404,
        "model=detectron2_fcos_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=detectron2_fcos_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.905546,
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 4.102073,
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 52.505858,
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 12.777115,
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 4.353643,
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 11.130121,
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 648.065479,
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=detectron2_maskrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 272.790808,
        "model=detectron2_maskrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=detectron2_maskrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6.082936,
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 89.346638,
        "model=detectron2_maskrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=detectron2_maskrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 95.580883,
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 71.854511,
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 54.592781,
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 109.925598,
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 27.247174,
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 9.102331,
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2375552,
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.961262,
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 826.78914,
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 125.044339,
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 10.86255,
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 15.447432,
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 78.212044,
        "model=detectron2_fasterrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=detectron2_fasterrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 12.667922,
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_50_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 109.418373,
        "model=detectron2_fasterrcnn_r_50_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=detectron2_fasterrcnn_r_50_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 3.116467,
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 64.346383,
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 4.658435,
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 27.561106,
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.569843,
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 179.101429,
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 0.997336,
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 87.408915,
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 78.154819,
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 27.373964,
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 18.472933,
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 0.942576,
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 11.243312,
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 296.625053,
        "model=detectron2_maskrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=detectron2_maskrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=detectron2_maskrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 113.848824,
        "model=detectron2_maskrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=detectron2_maskrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 23.072536,
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 37.717972,
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.44762,
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 22.757517,
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 13.471927,
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 690.762162,
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 4435.406076,
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.326159,
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.877943,
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 10.268085,
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 19.173461,
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 67.726226,
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 99.602791,
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.9944,
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.866142,
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 63.279469,
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 70.70118,
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 7.547972,
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "module 'numpy' has no attribute 'bool8'",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "module 'numpy' has no attribute 'bool8'",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "module 'numpy' has no attribute 'bool8'",
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 197.536431,
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 16.567535,
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 238.087095,
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 191.647465,
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 15.51052,
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.368072,
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.981152,
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 27.903005,
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 41.115336,
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 36.193021,
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_101_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 130.864873,
        "model=detectron2_fasterrcnn_r_101_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=detectron2_fasterrcnn_r_101_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6.702088,
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 408.488816,
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 16.123155,
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.25721599999999,
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed"
    }
}


# Extract only latency values
latencies = {}

for key, value in data["metrics"].items():
    if "metric=latencies" in key:
        model_name = key.split(",")[0].split("=")[1]  # Extract model name
        latencies[model_name] = value if isinstance(value, (int, float)) else -1  # Assign -1 for non-numeric values
            
sorted_latencies = {k: latencies[k] for k in sorted(latencies)}

# Print or use the extracted dictionary
print(sorted_latencies)



