import json

# Sample JSON data (replace with actual JSON)
data = {
    "metrics": {
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 0.9943,
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.837568000000005,
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 20.5278,
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.9887104,
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 105.5392,
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.98912,
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 67.8727,
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.98912,
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 356.0052,
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.9948544,
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 9.1606,
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.9948544,
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 3676.9433,
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.993216,
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 26.1868,
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.993216,
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 74.2199,
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.993216,
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8105.1045,
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 25.9981312,
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 121.137,
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0022272,
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 29.4034,
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0001792,
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 12.1496,
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0005888,
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "No module named 'triton'",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "No module named 'triton'",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "No module named 'triton'",
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 80.5256,
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0005888,
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.0526,
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0005888,
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 12.0295,
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0005888,
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 52.5351,
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0005888,
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 31.0361,
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0005888,
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 134.6809,
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.000998400000004,
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 9.6721,
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0018176,
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.0019,
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0018176,
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 932.4328,
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0018176,
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.2111,
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0018176,
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.5564,
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0018176,
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 21.9656,
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0022272,
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 178.3968,
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0022272,
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 213.8805,
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0022272,
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 10.7623,
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0026368,
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 17.7015,
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0026368,
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.4086,
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0026368,
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.1894,
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0026368,
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 94.61,
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0026368,
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 3.5215,
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0026368,
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 95.2666,
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.003046400000002,
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.1218,
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.003046400000002,
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 101.6388,
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.003456,
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 47.3676,
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0050944,
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 16044.5095,
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.004684800000003,
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "module 'numpy' has no attribute 'bool8'",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "module 'numpy' has no attribute 'bool8'",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "module 'numpy' has no attribute 'bool8'",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 5.5073,
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.004684800000003,
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 14.3405,
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.004684800000003,
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 13.1738,
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.004684800000003,
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 12.4616,
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.004684800000003,
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 4.2427,
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0050944,
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 9.2181,
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0050944,
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 19.164,
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0050944,
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 30.7242,
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0050944,
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 22.099,
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0059136,
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6260.2536,
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0063232,
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 222.5301,
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0063232,
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.0945,
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0063232,
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 7.1637,
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0063232,
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 130.0856,
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1479.8509,
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 17.7462,
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 15.8093,
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 42.1139,
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 3.03,
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 9.3958,
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 16.1108,
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 41.8196,
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.006732800000005,
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "Exceeded timeout: 3600",
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "Exceeded timeout: 3600",
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "Exceeded timeout: 3600",
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 27.8849,
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0091904,
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 31.8614,
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0091904,
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 3.8981,
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0096,
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2347.6628,
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0100096,
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 15.9439,
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0100096,
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 15.7579,
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0100096,
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2302.3392,
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0100096,
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 461.266,
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0100096,
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 7.9162,
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0100096,
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 12.7803,
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.010419200000005,
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 31.7404,
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.010419200000005,
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 225.4487,
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.010419200000005,
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 31.5056,
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.010419200000005,
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 61.767,
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0108288,
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 16.0214,
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 26.0108288,
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed"
    }


}


# Extract only latency values
latencies = {}

for key, value in data["metrics"].items():
    if "metric=latencies" in key:
        model_name = key.split(",")[0].split("=")[1]  # Extract model name
        latencies[model_name] = value if isinstance(value, (int, float)) else -1  # Assign -1 for non-numeric values
            
sorted_latencies = {k: latencies[k] for k in sorted(latencies)}

# Print or use the extracted dictionary
print(sorted_latencies)



