import json

# Sample JSON data (replace with actual JSON)
data = {
    "metrics": {
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=moco, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 18.595334,
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.0515968,
        "model=shufflenet_v2_x1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=mobilenet_v2_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 15.865797,
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 10.044395,
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=basic_gnn_gcn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.73379,
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=alexnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 267.217988,
        "model=detectron2_maskrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_maskrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=llama_v2_7b_16h, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 25.009426,
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=timm_efficientnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_101_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 128.567642,
        "model=detectron2_fasterrcnn_r_101_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_fasterrcnn_r_101_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 4.371452,
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=squeezenet1_1, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 79.606581,
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_GPT2_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=Background_Matting, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 26.635361,
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=pyhpc_isoneutral_mixing, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 27.023426,
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_T5, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=simple_gpt, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=resnet50_quantized_qat, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 27.000974,
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_Roberta_base, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 12.838932,
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=timm_vision_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 10.807676,
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=dlrm, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=simple_gpt_tp_manual, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.38337,
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=mobilenet_v2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.674367,
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=basic_gnn_gin, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 88.185085,
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=timm_vision_transformer_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 120.720592,
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=densenet121, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 686.137013,
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=sam, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 15.188482,
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_Albert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=functorch_maml_omniglot, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "(\"<class '_pickle.UnpicklingError'>\", \"('Weights only load failed. This file can still be loaded, to do so you have two options, \\\\x1b[1mdo those steps only if you trust the source of the checkpoint\\\\x1b[0m. \\\\n\\\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\\\n\\\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\\\n\\\\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\\\\n\\\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.',)\")",
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 63.137979,
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=torch_multimodal_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 199.598016,
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=vision_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.90237,
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=phlippe_resnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 110.691718,
        "model=detectron2_maskrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_maskrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 11.442846,
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=mnasnet1_0, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.622245,
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=opacus_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 3.981847,
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=basic_gnn_edgecnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 67.419325,
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=resnet152, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 221.089718,
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=nanogpt, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 639.62434,
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=tacotron2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.222984,
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=lennard_jones, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 22.574375,
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_Reformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6.536887,
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=llama, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 5.878646,
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=pytorch_CycleGAN_and_pix2pix, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 269.666936,
        "model=detectron2_fasterrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_fasterrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6.022077,
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_Whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 18.352312,
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=resnet50, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 11.571015,
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=pytorch_stargan, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 99.419404,
        "model=detectron2_fasterrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_fasterrcnn_r_101_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 86.701546,
        "model=detectron2_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_maskrcnn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 86.035867,
        "model=detectron2_maskrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_maskrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 61.141088,
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=fastNLP_Bert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1408.973768,
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_T5_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 4058.139905,
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=speech_transformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_50_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 105.2845,
        "model=detectron2_fasterrcnn_r_50_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_fasterrcnn_r_50_dc5, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 121.512794,
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=Super_SloMo, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 195.307288,
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_BigBird, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 237.907591,
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=demucs, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.774636,
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=functorch_dp_cifar10, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 16.02874,
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=phlippe_densenet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 16.712856,
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_GPT2, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_maskrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 288.710633,
        "model=detectron2_maskrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=detectron2_maskrcnn_r_101_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 113.575846,
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=microbench_unbacked_tolist_sum, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 6.747679,
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=doctr_reco_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 69.264387,
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=moondream, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 51.01316,
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=doctr_det_predictor, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "module 'numpy' has no attribute 'bool8'",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "module 'numpy' has no attribute 'bool8'",
        "model=soft_actor_critic, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "module 'numpy' has no attribute 'bool8'",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=stable_diffusion_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 174.320194,
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 37.2396032,
        "model=hf_Longformer, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=llava, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)",
        "model=llava, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)",
        "model=llava, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "Exceeded timeout: 10",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "Exceeded timeout: 10",
        "model=sam_fast, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "Exceeded timeout: 10",
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 92.858124,
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.144704,
        "model=pytorch_unet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 76.360879,
        "model=detectron2_fasterrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=detectron2_fasterrcnn_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 798.032365,
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=cm3leon_generate, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 75.596308,
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=hf_T5_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 3.159817,
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=resnet18, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 19.85456,
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=pyhpc_turbulent_kinetic_energy, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.340637,
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=dcgan, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fasterrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 248.247951,
        "model=detectron2_fasterrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=detectron2_fasterrcnn_r_50_c4, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 26.489781,
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=timm_vovnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 54.08671,
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=timm_nfnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 388.660022,
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=maml, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 10.310139,
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=pyhpc_equation_of_state, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 18.806411,
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=BERT_pytorch, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 40.577529,
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=hf_distil_whisper, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 10.760384,
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=resnext50_32x4d, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=detectron2_fcos_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 75.869702,
        "model=detectron2_fcos_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=detectron2_fcos_r_50_fpn, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 97.696294,
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=yolov3, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 23.081739,
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=hf_clip, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.809824,
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=tts_angular, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 36.304955,
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=timm_regnet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 37.376255,
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=hf_Bert_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 13.230245,
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=hf_Bart, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=stable_diffusion_text_encoder, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 2.684548,
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=vgg16, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 70.643047,
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=hf_T5_base, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 0.681685,
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=nvidia_deeprecommender, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.542147,
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=timm_resnest, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.991405,
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=drq, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": "not_implemented",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": "not_implemented",
        "model=timm_efficientdet, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "not_implemented",
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 8.738212,
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=hf_DistilBert, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 5929.577012,
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=LearningToPaint, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 11.609493,
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=mobilenet_v3_large, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed",
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=latencies": 1.365281,
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=cpu_peak_mem": 19.3736704,
        "model=basic_gnn_sage, test=eval, device=xpu, bs=None, extra_args=[], metric=gpu_peak_mem": "failed"
    }
}


# Extract only latency values
latencies = {}

for key, value in data["metrics"].items():
    if "metric=latencies" in key:
        model_name = key.split(",")[0].split("=")[1]  # Extract model name
        latencies[model_name] = value if isinstance(value, (int, float)) else -1  # Assign -1 for non-numeric values
            
sorted_latencies = {k: latencies[k] for k in sorted(latencies)}

# Print or use the extracted dictionary
print(sorted_latencies)



